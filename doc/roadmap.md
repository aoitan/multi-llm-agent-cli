# 開発ロードマップ

## フェーズ1: 基本機能の実装

### 1.1 Ollama APIとの連携
- Ollama APIを通じてLLMとチャットを行う機能の実装
- Ollama Librariesにある任意のモデルを利用できる機能の実装

### 1.2 基本的なCLIインターフェース
- コマンドラインからの入力受付とLLM応答の表示
- モデル選択機能

## フェーズ2: コンテキスト管理とMCP対応

### 2.1 コンテキスト管理
- LLMとの会話からコンテキストを管理する機能
    - コンテキストの記憶、破棄、要約の仕組み
- コンテキスト管理の戦略（例: 過去Nターンの会話を保持、特定のキーワードを含む会話を保持など）

### 2.2 MCP (Model Context Protocol)
- MCPサーバーとLLM、ユーザーの仲立ちを行うMCPクライアント機能の実装

## フェーズ3: 高度な機能と拡張性

### 3.1 複数Ollamaエンドポイント対応
- 複数のOllamaエンドポイントを登録・切り替えできる機能
- 複数エンドポイントのオーケストレーション（初期はラウンドロビン）

### 3.2 高度なオーケストレーション
- 指示者LLMと作業者LLMを設定し、協調作業させる機能（将来的な展望）

### 3.3 プラグイン/拡張機能
- 外部ツール連携やカスタム機能を追加できるプラグイン機構の検討

## フェーズ4: 安定化と改善

### 4.1 エラーハンドリングと堅牢性
- APIエラー、ネットワークエラーなどに対する適切なエラーハンドリング

### 4.2 パフォーマンス最適化
- レスポンス速度の向上、リソース使用量の最適化

### 4.3 ドキュメンテーションとテスト
- ユーザー向けドキュメントの整備
- 単体テスト、結合テストの拡充
# MCPサーバーとLLM（Ollama）連携の設計

## 概要
MCPサーバーがMCPクライアントからのリクエストを受け取り、Ollama APIを通じてLLMと対話し、その応答をクライアントに返す仕組みを設計します。

## 連携フロー
1.  **クライアントからのリクエスト**: MCPクライアントは`user_message`メソッド（JSON-RPCリクエスト）をMCPサーバーに送信します。このリクエストには、ユーザーのプロンプト、使用するLLMモデル名（オプション）、および関連するコンテキストIDが含まれます。
2.  **サーバーでの処理**: MCPサーバーは`user_message`リクエストを受け取ると、以下の処理を行います。
    *   リクエストからプロンプトとモデル名を抽出します。
    *   内部で保持している`OllamaClient`インスタンスを使用して、Ollama APIの`chat`メソッドを呼び出します。
    *   この際、MCPサーバーはコンテキスト管理の役割も担うため、過去の会話履歴を`OllamaClient`に渡す必要があります。これは、`user_message`リクエストに含まれる`context_id`に基づいて、サーバーが管理する会話履歴から取得します。
3.  **Ollamaとの対話**: `OllamaClient`はOllama APIにリクエストを送信し、LLMからのストリーミング応答を受け取ります。
4.  **応答の転送**: MCPサーバーは`OllamaClient`から受け取ったストリーミング応答のチャンクを、MCPクライアントに対して`llm_response_chunk`通知（JSON-RPC通知）としてリアルタイムに転送します。各チャンクにはLLMの応答の一部が含まれ、最後のチャンクには`done: true`が設定されます。
5.  **エラーハンドリング**: Ollama APIからのエラーや、通信中のエラーが発生した場合は、MCPサーバーは適切なJSON-RPCエラーレスポンスをクライアントに返します。

## コンテキスト管理
MCPサーバーは、各クライアントセッションまたはコンテキストIDごとに会話履歴を管理します。`user_message`リクエストが来るたびに、そのメッセージを履歴に追加し、LLMに送信する際には関連する履歴を含めます。将来的には、この履歴の要約や永続化も検討します。

## 実装の詳細
-   `McpServer`クラスに`OllamaClient`のインスタンスを追加します。
-   `handleMessage`メソッド内で`user_message`メソッドを処理するロジックを追加します。
-   `OllamaClient.chat`からのストリーミング応答を処理し、`llm_response_chunk`通知としてクライアントに送信する非同期イテレータの処理を実装します。
-   会話履歴を管理するためのデータ構造（例: `Map<string, Message[]>`）を`McpServer`内に定義します。

## 今後の検討事項
-   コンテキストの永続化メカニズム。
-   複数のLLMモデルやOllamaエンドポイントの管理。
-   ツール利用のためのLLM連携（Function Callingなど）。
